{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.&nbsp;Connect G-Drive"
      ],
      "metadata": {
        "id": "iXzJbbuRTyIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NWSeXQhqTV4g",
        "outputId": "f834c2b6-35d2-47f2-93d9-05e98932353e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "default_dir = \"/content/drive/MyDrive/...\"\n",
        "os.chdir(default_dir)"
      ],
      "metadata": {
        "id": "gg0k1FnuTVTE",
        "outputId": "212d9555-fe0c-4d08-dd79-abf7138d678d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/...'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d8f03ac1b333>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdefault_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/...'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "esjFuZG2SW0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "XlBuBaA-XOjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.&nbsp;Import Libraries"
      ],
      "metadata": {
        "id": "nojV_tYxUwRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# text processing\n",
        "import re\n",
        "import nltk"
      ],
      "metadata": {
        "id": "7tJpAo1vU0ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "-7a7WhXsfzbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "CnIvxan2giHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "zTnvZPfxlCmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "yZAokwzjnJKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "iNqVCPo4nKO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "cq-2Q2_InWhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "IFAMEzyfrSL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Modeling\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "oeiz0eJss0g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model Evaluation\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "buWmbWTKty6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.&nbsp;Load Dataset"
      ],
      "metadata": {
        "id": "gkxE8YB8Vi1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_spam = pd.read_csv('spam.csv', encoding=\"latin-1\")\n",
        "df_spam.head()"
      ],
      "metadata": {
        "id": "pvlYz99MTVDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop un-meaningful columns\n",
        "\n",
        "df_spam = df_spam.drop(\n",
        "    columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
        ")\n",
        "\n",
        "df_spam.head()"
      ],
      "metadata": {
        "id": "iGUUJB4nbbcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename columns\n",
        "\n",
        "df_spam = df_spam.rename(\n",
        "    columns={\n",
        "        'v1': 'label',\n",
        "        'v2': 'message'\n",
        "    }\n",
        ")\n",
        "\n",
        "df_spam.head()"
      ],
      "metadata": {
        "id": "cRcbUqokcCAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.&nbsp; Dataset Exploration"
      ],
      "metadata": {
        "id": "qg8CqIhlXKw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.1. Label Counts"
      ],
      "metadata": {
        "id": "f6coHmxIYQHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_spam.label.value_counts())\n",
        "df_spam.label.value_counts().plot(kind='bar');"
      ],
      "metadata": {
        "id": "YAsgZqO-eLPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.2. Message Length"
      ],
      "metadata": {
        "id": "CXxfvN_7YS2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_spam['message_len'] = df_spam.message.apply(len)\n",
        "\n",
        "df_spam.sort_values(by='message_len', ascending=False).head(10)"
      ],
      "metadata": {
        "id": "zjyTPw-EeSGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.&nbsp;Text Preprocessing"
      ],
      "metadata": {
        "id": "T3i6Jm6yZH9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.1. Text Cleaning"
      ],
      "metadata": {
        "id": "qsZo6ocXdhrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_text = \"Nama saya HENDY\""
      ],
      "metadata": {
        "id": "xpOkau8yCeBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_text.split()"
      ],
      "metadata": {
        "id": "awROeenOCj-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"#\".join([\"nama\", \"saya\", \"hendy\"])"
      ],
      "metadata": {
        "id": "dlb-aLbdC4S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to clean up the text\n",
        "def clean_text(text):\n",
        "\n",
        "    # Replacing all non-alphabetic characters with a space\n",
        "    sms = re.sub('[^a-zA-Z]', ' ', text)\n",
        "\n",
        "    # Converting to lowecase\n",
        "    sms = sms.lower()\n",
        "\n",
        "    # Splitting text (check the defaults of split func!)\n",
        "    sms = sms.split()\n",
        "\n",
        "    # Rejoining text\n",
        "    sms = ' '.join(sms)\n",
        "\n",
        "    return sms"
      ],
      "metadata": {
        "id": "yUB_dr7Eeeqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spam[\"cleaned_text\"] = df_spam[\"message\"].apply(clean_text)\n",
        "\n",
        "# Lets have a look at a sample of texts after cleaning\n",
        "print(\"The First 10 Texts after cleaning: \\n\")\n",
        "print(*df_spam[\"cleaned_text\"][:10], sep=\"\\n\")"
      ],
      "metadata": {
        "id": "aIKieTrh9pWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_spam['message'][0])\n",
        "print(df_spam['cleaned_text'][0])"
      ],
      "metadata": {
        "id": "cUAquqj_PkLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.2. Tokenization"
      ],
      "metadata": {
        "id": "txzZL0K0hPLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_spam[\"tokenized_text\"] = df_spam.apply(\n",
        "    lambda row: nltk.word_tokenize(row[\"cleaned_text\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "df_spam.head(10)"
      ],
      "metadata": {
        "id": "gtGSVk-qe1Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.3. Remove Stopwords"
      ],
      "metadata": {
        "id": "8fZoDcM1lnZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the stopwords function\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered_text = [word for word in text if word not in stop_words]\n",
        "    return filtered_text"
      ],
      "metadata": {
        "id": "wNwj1E0ZfFun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spam[\"no_stopword_text\"] = df_spam[\"tokenized_text\"].apply(remove_stopwords)\n",
        "\n",
        "print(\"The First 10 Texts after removing the stopwords: \\n\")\n",
        "print(*df_spam[\"no_stopword_text\"][:10], sep=\"\\n\")"
      ],
      "metadata": {
        "id": "IZu9RrLYl9VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spam"
      ],
      "metadata": {
        "id": "lJyz3u0Ax4O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.4. Lemmatization"
      ],
      "metadata": {
        "id": "DrSLiQA0nbPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# lemmatize string function\n",
        "def lemmatize_word(text):\n",
        "    # provide context i.e. part-of-speech (pos)\n",
        "    lemmas = [lemmatizer.lemmatize(word, pos='v') for word in text]\n",
        "    return lemmas"
      ],
      "metadata": {
        "id": "GM8qRbVihJ1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spam[\"lemmatized_text\"] = df_spam[\"no_stopword_text\"].apply(lemmatize_word)\n",
        "\n",
        "print(\"The First 10 Texts after lemitization: \\n\")\n",
        "print(*df_spam[\"lemmatized_text\"][:10], sep=\"\\n\")"
      ],
      "metadata": {
        "id": "y_MsUg_Mn2L0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spam"
      ],
      "metadata": {
        "id": "B82-oG0DqXK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.5. Vectorization"
      ],
      "metadata": {
        "id": "auS3SAoBn4Lq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.5.1. Creating a corpus of lemmatized text"
      ],
      "metadata": {
        "id": "xazZqc8ap_Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a corpus of text feature to encode further into vectorized form\n",
        "corpus = []\n",
        "for i in df_spam[\"lemmatized_text\"]:\n",
        "    msg = ' '.join([row for row in i])\n",
        "    corpus.append(msg)\n",
        "\n",
        "print(\"The First 10 lines in corpus : \\n\")\n",
        "print(*corpus[:10], sep=\"\\n\")"
      ],
      "metadata": {
        "id": "bvjuRpTShto4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spam.head()"
      ],
      "metadata": {
        "id": "_h5fWY_aFBNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.5.2. Converting the corpus in vector form"
      ],
      "metadata": {
        "id": "3z5DGeJprX0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing text data in to numbers.\n",
        "tfidf = TfidfVectorizer()\n",
        "X = tfidf.fit_transform(corpus).toarray()\n",
        "\n",
        "# Let's have a look at our feature\n",
        "X.dtype"
      ],
      "metadata": {
        "id": "_ykQwwwBh_gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "0HOO2AnajG8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "6mxcaCnpFh1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_spam)"
      ],
      "metadata": {
        "id": "4VtBtmtBFr9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0, :]"
      ],
      "metadata": {
        "id": "o7BINA0VRdrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.5.5. Label Encoding the classes in Target"
      ],
      "metadata": {
        "id": "IPE2JMMFsSiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label encode the Target and use it as y\n",
        "le = LabelEncoder()\n",
        "df_spam[\"label\"] = le.fit_transform(df_spam[\"label\"])"
      ],
      "metadata": {
        "id": "MCrJiM4d-aUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spam.head()"
      ],
      "metadata": {
        "id": "PQd0hdErAJuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6.&nbsp;Model Building"
      ],
      "metadata": {
        "id": "0h9fBSfmsfV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting values for labels and feature as y and X (we already did X in vectorizing...)\n",
        "y = df_spam[\"label\"]\n",
        "\n",
        "# Splitting the testing and training sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "dPB6rkUQALPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing on the following classifiers\n",
        "classifiers = [\n",
        "    MultinomialNB(),\n",
        "    RandomForestClassifier(),\n",
        "    KNeighborsClassifier(),\n",
        "    SVC()\n",
        "]\n",
        "\n",
        "for cls in classifiers:\n",
        "    cls.fit(X_train, y_train)\n",
        "\n",
        "# Dictionary of pipelines and model types for ease of reference\n",
        "pipe_dict = {\n",
        "    0: \"NaiveBayes\",\n",
        "    1: \"RandomForest\",\n",
        "    2: \"KNeighbours\",\n",
        "    3: \"SVC\"\n",
        "}"
      ],
      "metadata": {
        "id": "DC-0MiPuAW_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.&nbsp;Model Evaluation"
      ],
      "metadata": {
        "id": "J-u8OG91uqkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-Validation\n",
        "for i, model in enumerate(classifiers):\n",
        "    cv_score = cross_val_score(model, X_train,y_train, scoring=\"accuracy\", cv=10)\n",
        "    print(\"%s: %f \" % (pipe_dict[i], cv_score.mean()))"
      ],
      "metadata": {
        "id": "ietBc3TpAcL1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}